{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kurset gennemgår grundlæggende Python-kode, der kan få dig i gang med at bruge programmering som redskab til tekstbehandling, kvantitative analyser og tekst- og datamining.\n",
    "\n",
    "Mere teknisk fortalt gennemgår vi begreber som variabler, værdier, datatyperne tekststrenge og lister, loops og import og brug af pakker, f.eks. NLTK-pakken. Samtidigt lærer du om python-programmet Jupyter Notebook, så opnår du også et kendskab til det."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import af biblioteker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I python importerer man ofte biblioteker for at supplere med nogle flere funktioner. Det er smart, fordi det er huritgere at programmere, når man ikke skal programmere alle funktionerne fra bunden. Der følger mange biblioteker med, når man installerer Anaconda. Bibliotekerne bliver også nogle gange kaldt for pakker eller moduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.gutenberg.org/files/84/84-0.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = urllib.request.urlopen(url).read().decode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "raw_text variablen er en 'string.' \n",
    "\n",
    "En string laver man ved at bruge citationstegn. Man kan enten bruge enkelt sitationstegn eller dobbelt citationstegn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ved at buge metoden find('fiktiv tekststreng') kan vi få retuneret et indextal, der er lig med det indextal, hvor 'fiktiv tekststreng' begynder.\n",
    "\n",
    "Neden for bruger vi .find() til at finde de indextal, hvor teksten starter og slutter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_start = raw_text.find('*** START OF THE PROJECT GUTENBERG EBOOK FRANKENSTEIN ***')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_start = text_start + len('*** START OF THE PROJECT GUTENBERG EBOOK FRANKENSTEIN ***')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "text_end = raw_text.find('*** END OF THE PROJECT GUTENBERG EBOOK FRANKENSTEIN ***')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vælg et udsnit af teksten\n",
    "\n",
    "Hvert enkelt tegn i en string (tekststeng) har et indekstal. Bemærk at i Python er første indextal 0 og ikke 1. Vi kan hente en udsnit af tekststrengen ved at skrive string[første indextal : andet indextal].\n",
    "\n",
    "F.eks. retunerer string[0:50] de første 50 tegn.\n",
    "\n",
    "Vi kan også bruge negative tal.\n",
    "\n",
    "F.eks. retunerer string[-50:] de sidste 50 tegn.\n",
    "\n",
    "Nedenfor bruger vi variabler vis værdi er lig med et tal i stedet for et tal. Det går nemlig også. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = raw_text[text_start:text_end].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[500:3500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text[500:3500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opgave: prøv at finde et nyt start sted, som ligger efter indholdsfortegnelsen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rensning af tekst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kodenstykket neden for bruger vi til at rense teksten op for alle andre tegn end bogstaver. Koden gør brug af biblioteket 're', som næste kursus kommer til at handle om. Derfor springer vi overforklaringen på, hvad der sker i koden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text_clean = text.replace('_','')\n",
    "text_clean = ' '.join(re.findall(r'\\b(\\S+)\\b', text_clean) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lister"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lister bliver benyttet til at gemme flere værdier i en variabel. Neden for bruger vi nltk.word_tokenize(), der retunerer en liste.For at bruge nltk.word_tokenize() skal vi først importere biblioteket nltk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenisering af tekst betyder en opslitning af teksten til en liste af ord.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "tokenized_text = nltk.word_tokenize(text_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Som det fremgår ovenfor bliver lister lavet vha. firkantede parenteser ( [ ] ).\n",
    "\n",
    "Man kan tilgå elementerne i listen ved at referere til indekstallet. Igen kan vi bruge både positive og nagative tal. Husk at i python er første indextal 0 og ikke 1, hvilket betyder, at vi tilgår det første og det sidste element i listen på denne måde:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (tokenized_text[0])\n",
    "print (tokenized_text[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forbered teksten til nltk metoderne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For at kunne arbejde med vores tekstdata skal vi bearbejde vores tekststreng lidt.\n",
    "\n",
    "Først konverterer vi teksten til en liste over tokens med nltk word_tokenize()-funktionen (det har vi gjort ovenfor). Vi opretter også et nltk-tekstobjekt, som giver os mulighed for at anvende forskellige nltk-metoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text = nltk.Text(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nltk-tekstobjektet bliver produceret fra en list af tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts of Speech Tagging (POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tagging er grammatisk tagging. Der bliver tildelt et grammatisk tag til hvert ord. For at bruge nltk's pos tagger forudsætter det at vi har et nltk tekstobjekt, som vi har produceret ovenfor.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part-of-speech tagging af ord i tokeniserede afsnit\n",
    "tagged_text = nltk.pos_tag(nltk_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loops bruger man til at gentage den samme handling. I denne sammenhæng vil vi prøve at bruge et loop til gennemse listen tagged_text for et udvalgt POS tag.\n",
    "\n",
    "Vi begynder med en tom liste. Lister laver vi med firkantede parenteser.\n",
    "\n",
    "Til loopets syntaks kan vi oversætte til: ' For hver element i variablen tagged_text, hvis elementets anden værdi er lige med 'NN', så tag den tomme liste og tilføj elementets første værdi dertil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_tags = []\n",
    "for item in tagged_text:\n",
    "    if item[1] == 'NN':\n",
    "        nn_tags.append(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lad os se på fordelingen af tags\n",
    "nltk.FreqDist(nn_tags).plot(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opgave: prøv at udskifte 'NN' med andre tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named entity recognition (NER)\n",
    "\n",
    "Chunk tagged text returner en nltk class, der består af POS tag og NER tags (person, lokation og organisation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_text = nltk.ne_chunk(tagged_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_persons = []\n",
    "for tree in chunked_text:\n",
    "    if type(tree) is nltk.tree.Tree and tree.label() == \"PERSON\":\n",
    "            label  = \" \".join([word for word, pos in tree])\n",
    "            all_persons.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_locations = []\n",
    "for tree in chunked_text:\n",
    "    if type(tree) is nltk.tree.Tree and tree.label() == \"LOCATION\":\n",
    "            label  = \" \".join([word for word, pos in tree])\n",
    "            all_locations.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_organizations = []\n",
    "for tree in chunked_text:\n",
    "    if type(tree) is nltk.tree.Tree and tree.label() == \"ORGANIZATION\":\n",
    "            label  = \" \".join([word for word, pos in tree])\n",
    "            all_organizations.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lad os se på person galleriet\n",
    "nltk.FreqDist(all_persons).plot(20, title='Personer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lad os se hvor romanen foregår\n",
    "nltk.FreqDist(all_locations).plot(20, title='Lokationer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lad os se hvor romanen foregår\n",
    "nltk.FreqDist(all_organizations).plot(20, title='Organisationer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopord\n",
    "\n",
    "Stopord er småord, som ofte ikke er betydningsbærende ord.\n",
    "\n",
    "Vi har defor brug for at indlæse en stopordsliste, og den har vi heldigvis lige ved hånden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu kan alle teksterne bliver filtreret for stopord."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokens = []\n",
    "for word in nltk_text:\n",
    "    if word.lower() not in stopwords and word.isalpha():\n",
    "        filtered_tokens.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Med en ny ordliste, der ikke længere indeholder stopord, kan vi få overblik over, hvilke betydningsbærende ord, der flyder mest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_filtered = nltk.FreqDist(filtered_tokens).plot(20, title='Hyppigste ord (uden stopord)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_tokens = []\n",
    "\n",
    "for word in filtered_tokens:\n",
    "    if len(word) > 10:\n",
    "        long_tokens.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_filtered = nltk.FreqDist(long_tokens).plot(20, title='Længste ord')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK metoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collocation_list() returnerer en liste over de mest almindelige ordpar i teksten. Bemærk, at i nogle versioner af Python virker collocation_list() ikke. Hvis dette er tilfældet, prøv _collocations()_ i stedet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text.collocation_list(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concordance()-metoden returnerer konteksten af et specifikt udtryk. Længden af output kan ændres med parametrene i width og lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text.concordance('natural', lines=30, width=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For at identificere ord, der optræder i en lignende kontekst, kan vi bruge metoden similar()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text.similar('God')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metoden dispersion_plot() lader os visualisere, hvordan termer forekommer på tværs af vores tekst. Metoden accepterer en liste med et eller flere udtryk som input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = ['winter', 'weather', 'dark', 'rain']\n",
    "\n",
    "nltk_text.dispersion_plot(terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate() metoden kan du bruge til at genere mere eller mindre sammenhængende tekst med udgangspunkt i en eksisterende tekst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_gen = nltk_text.generate(150)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
